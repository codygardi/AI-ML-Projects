{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef9dab82",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP): Sentiment Analysis\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, we‚Äôll perform **Sentiment Analysis** ‚Äî a foundational task in NLP that determines whether a piece of text expresses a **positive**, **negative**, or **neutral** opinion.  \n",
    "We‚Äôll use three approaches to show the evolution of NLP models:\n",
    "\n",
    "1. **Naive Bayes** ‚Äî a simple statistical model that assumes word independence.  \n",
    "2. **Logistic Regression** ‚Äî a linear classifier that uses word frequencies or TF-IDF features.  \n",
    "3. **Transformer (BERT)** ‚Äî a pre-trained deep learning model that understands context at the sentence level.\n",
    "\n",
    "The goal is to classify text reviews and compare how traditional machine learning stacks up against modern transformers built with **PyTorch**.\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "**Dataset Name:** IMDb Movie Reviews Dataset  \n",
    "**Source:** [Kaggle IMDb Dataset of 50K Movie Reviews](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)  \n",
    "**Size:** 50,000 labeled reviews (balanced 25K positive, 25K negative)  \n",
    "**Target Variable:** `sentiment` (values: *positive* or *negative*)\n",
    "\n",
    "Each review contains raw English text written by users.  \n",
    "We‚Äôll preprocess the data (clean text, tokenize, and remove stopwords), vectorize it using TF-IDF, and then train and evaluate the models.\n",
    "\n",
    "---\n",
    "\n",
    "## Objective and Predictions\n",
    "\n",
    "The objective is to **predict sentiment polarity** based on review text.  \n",
    "We‚Äôll compare:\n",
    "- Accuracy, precision, recall, and F1-score of each model.  \n",
    "- Model interpretability (which words drive predictions).  \n",
    "- Training speed and computational efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "## Hypothesized Conclusions\n",
    "\n",
    "1. **Naive Bayes** will perform surprisingly well for its simplicity, reaching ~85‚Äì88% accuracy.  \n",
    "2. **Logistic Regression** will outperform Naive Bayes slightly, as it better models correlated word features.  \n",
    "3. **BERT (PyTorch)** will achieve the best accuracy (~93‚Äì96%) by understanding context, negations, and tone.\n",
    "\n",
    "---\n",
    "\n",
    "## Why We Use These Models for This Dataset\n",
    "\n",
    "| Model | Why It‚Äôs Used |\n",
    "|--------|----------------|\n",
    "| **Naive Bayes** | Simple baseline for word-based classification. Great for bag-of-words features. |\n",
    "| **Logistic Regression** | Improves on Naive Bayes by weighting words more flexibly. |\n",
    "| **BERT (Transformer)** | Understands full sentence meaning and context; ideal for nuanced human language. |\n",
    "\n",
    "In simple terms:\n",
    "- Naive Bayes counts word probabilities.\n",
    "- Logistic Regression balances words mathematically.\n",
    "- BERT *understands* what you mean.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook will go through:\n",
    "1. Data loading and exploration  \n",
    "2. Text preprocessing and tokenization  \n",
    "3. Model training (Naive Bayes ‚Üí Logistic Regression ‚Üí BERT with PyTorch)  \n",
    "4. Evaluation and comparison of results  \n",
    "\n",
    "When you‚Äôre ready, we‚Äôll begin by **loading and inspecting the dataset** from your `data/` directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa778fa",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279a7253",
   "metadata": {},
   "source": [
    "## Data Loading and Initial Exploration\n",
    "\n",
    "We'll begin by loading the **IMDb Movie Reviews** dataset from the local `data/` directory.  \n",
    "This dataset contains 50,000 movie reviews labeled as positive or negative.  \n",
    "Before we jump into preprocessing, we‚Äôll inspect a few samples, check label distribution, and ensure that text lengths are balanced enough for modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99c2f4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (50000, 2)\n",
      "\n",
      "Columns: ['review', 'sentiment']\n",
      "\n",
      "Missing Values: 0\n",
      "\n",
      "Sentiment Distribution:\n",
      "sentiment\n",
      "positive    50.0\n",
      "negative    50.0\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load IMDb dataset\n",
    "df = pd.read_csv(\"data/IMDB Dataset.csv\")\n",
    "\n",
    "# Basic overview\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nColumns:\", df.columns.tolist())\n",
    "print(\"\\nMissing Values:\", df.isnull().sum().sum())\n",
    "\n",
    "# Check sentiment distribution\n",
    "print(\"\\nSentiment Distribution:\")\n",
    "print(df['sentiment'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Display a few examples\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b015d217",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "- The dataset contains **50,000 reviews** evenly split between positive and negative sentiment ‚Äî perfect for binary classification.  \n",
    "- Each entry includes raw, unprocessed English text. Some reviews may contain punctuation, HTML tags, or mixed casing.  \n",
    "- There are **no missing values**, so we can move directly to cleaning and preparing the text.\n",
    "\n",
    "Next, we‚Äôll preprocess the data ‚Äî cleaning, tokenizing, and vectorizing the text so models can learn from it effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d459df2",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81cc954",
   "metadata": {},
   "source": [
    "## Text Preprocessing and Vectorization\n",
    "\n",
    "Machine learning models can‚Äôt directly understand raw text ‚Äî we need to **convert words into numbers**.  \n",
    "We‚Äôll clean the reviews by:\n",
    "1. Lowercasing text  \n",
    "2. Removing punctuation, HTML tags, and special symbols  \n",
    "3. Tokenizing words (splitting text into individual words)  \n",
    "4. Removing common stopwords (like ‚Äúthe‚Äù, ‚Äúand‚Äù, ‚Äúis‚Äù)  \n",
    "5. Converting words into TF-IDF vectors ‚Äî a numerical format representing how important each word is across all reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed9881a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gardi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF feature matrix shape: (40000, 10000)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download stopwords if needed\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<.*?>', '', text)  # remove HTML tags\n",
    "    text = text.lower()                # lowercase\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)  # remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)    # remove digits\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])  # remove stopwords\n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "df['cleaned_review'] = df['review'].apply(clean_text)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_review'], df['sentiment'], \n",
    "                                                    test_size=0.2, random_state=42, stratify=df['sentiment'])\n",
    "\n",
    "# TF-IDF vectorization\n",
    "tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(\"TF-IDF feature matrix shape:\", X_train_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f832812",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Now every review is represented as a **TF-IDF feature vector** ‚Äî essentially a long list of word importance scores.  \n",
    "The model can now recognize which words are strong indicators of positive or negative sentiment (like *‚Äúamazing‚Äù*, *‚Äúawful‚Äù*, *‚Äúboring‚Äù*, etc.).\n",
    "\n",
    "In short, we‚Äôve turned messy English text into structured numerical data the models can actually learn from.\n",
    "\n",
    "Next, we‚Äôll train a **Naive Bayes classifier** ‚Äî a fast, classic baseline model for text classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48431a49",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039b0c68",
   "metadata": {},
   "source": [
    "## Naive Bayes Model ‚Äî Statistical Baseline for Text Classification\n",
    "\n",
    "We‚Äôll start with **Multinomial Naive Bayes**, one of the most popular and effective algorithms for text classification.  \n",
    "It works by using **word frequencies** to calculate the probability that a review belongs to a certain class (positive or negative).  \n",
    "\n",
    "Even though it assumes all words are independent (which isn‚Äôt true in real language), Naive Bayes often performs surprisingly well on large text datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54f9d46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.8655\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.85      0.86      5000\n",
      "    positive       0.86      0.88      0.87      5000\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[4254  746]\n",
      " [ 599 4401]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Initialize and train Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_nb = nb.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluation\n",
    "acc_nb = accuracy_score(y_test, y_pred_nb)\n",
    "print(f\"Naive Bayes Accuracy: {acc_nb:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_nb = confusion_matrix(y_test, y_pred_nb)\n",
    "print(\"Confusion Matrix:\\n\", cm_nb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babab189",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Typical performance for Naive Bayes on IMDb reviews is around **85‚Äì88% accuracy**.  \n",
    "The model learns to associate positive words (like *‚Äúgreat‚Äù*, *‚Äúexcellent‚Äù*, *‚Äúlove‚Äù*) with positive sentiment and negative words (like *‚Äúbad‚Äù*, *‚Äúboring‚Äù*, *‚Äúterrible‚Äù*) with negative sentiment.\n",
    "\n",
    "In simple terms:  \n",
    "Naive Bayes reads a review like a word counter ‚Äî it tallies positive and negative words and chooses whichever side wins.  \n",
    "\n",
    "While fast and interpretable, it doesn‚Äôt understand word order or context (e.g., *‚Äúnot good‚Äù* still looks positive to it).\n",
    "\n",
    "Next, we‚Äôll improve on this with **Logistic Regression**, which gives each word a learned weight instead of assuming equal importance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cec80a",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633ad397",
   "metadata": {},
   "source": [
    "## Logistic Regression ‚Äî Weighted Word Importance Model\n",
    "\n",
    "While Naive Bayes treats all words independently, **Logistic Regression** learns a **weight** for each word or phrase (n-gram) to better capture subtle relationships.  \n",
    "For example, it can tell that ‚Äúnot good‚Äù means something different from ‚Äúgood‚Äù ‚Äî which Naive Bayes can‚Äôt do.\n",
    "\n",
    "We‚Äôll use the same TF-IDF features but train a linear model that directly optimizes classification accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10405f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.8958\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.89      0.89      5000\n",
      "    positive       0.89      0.91      0.90      5000\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[4427  573]\n",
      " [ 469 4531]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize and train Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=200, n_jobs=-1)\n",
    "log_reg.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lr = log_reg.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluation\n",
    "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
    "print(f\"Logistic Regression Accuracy: {acc_lr:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "print(\"Confusion Matrix:\\n\", cm_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69baaae9",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Logistic Regression typically achieves **88‚Äì90% accuracy**, slightly outperforming Naive Bayes.  \n",
    "It gives **each word a unique learned weight**, allowing it to distinguish between nuanced expressions such as ‚Äúnot great‚Äù vs. ‚Äúgreat.‚Äù\n",
    "\n",
    "In simpler terms:  \n",
    "Naive Bayes votes based on word counts,  \n",
    "while Logistic Regression *learns how strongly each word pushes the review toward positive or negative.*\n",
    "\n",
    "This model is still lightweight and interpretable, making it a strong choice for production systems needing transparency.\n",
    "\n",
    "Next, we‚Äôll step up to **BERT (Transformer)** using **PyTorch**, a modern deep learning model that understands language context, meaning, and tone at a much deeper level.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03929d0b",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cf4b98",
   "metadata": {},
   "source": [
    "## Transformer Model (BERT) ‚Äî Contextual Deep Learning for Sentiment Analysis\n",
    "\n",
    "Now we‚Äôll use **BERT (Bidirectional Encoder Representations from Transformers)** ‚Äî a state-of-the-art NLP model from Google, built on the Transformer architecture.  \n",
    "Unlike traditional models, BERT reads text **in both directions** (left-to-right and right-to-left), understanding *context* rather than just word frequency.\n",
    "\n",
    "We‚Äôll fine-tune a pre-trained BERT model using **PyTorch** and **Hugging Face Transformers**, leveraging modern optimizers like **AdamW replaced with torch.optim.Adam** (the current best practice).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dac05660",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio transformers --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c03d136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Sample subset for training (to fit in notebook memory)\n",
    "df_sample = df.sample(10000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Tokenizer and encoding\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "encodings = tokenizer(list(df_sample['cleaned_review']), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Convert sentiments to binary labels\n",
    "labels = torch.tensor(df_sample['sentiment'].map({'negative': 0, 'positive': 1}).values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b774ae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom PyTorch Dataset\n",
    "class IMDbDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "dataset = IMDbDataset(encodings, labels)\n",
    "\n",
    "# Train/test split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "954beea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]C:\\Users\\Gardi\\AppData\\Local\\Temp\\ipykernel_15800\\1113597505.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx])\n",
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [38:55<00:00,  2.34s/it, loss=0.518]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [39:38<00:00,  2.38s/it, loss=0.109] \n"
     ]
    }
   ],
   "source": [
    "# Initialize BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer (AdamW deprecated ‚Üí use Adam)\n",
    "optimizer = Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "epochs = 2\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loop.set_description(f\"Epoch {epoch+1}\")\n",
    "        loop.set_postfix(loss=loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18bcd3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]C:\\Users\\Gardi\\AppData\\Local\\Temp\\ipykernel_15800\\1113597505.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx])\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [02:20<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Accuracy: 0.8540\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.79      0.84       985\n",
      "           1       0.82      0.92      0.86      1015\n",
      "\n",
      "    accuracy                           0.85      2000\n",
      "   macro avg       0.86      0.85      0.85      2000\n",
      "weighted avg       0.86      0.85      0.85      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "preds, truths = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        preds.extend(predictions.cpu().numpy())\n",
    "        truths.extend(labels.cpu().numpy())\n",
    "\n",
    "# Metrics\n",
    "acc_bert = accuracy_score(truths, preds)\n",
    "print(f\"BERT Accuracy: {acc_bert:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(truths, preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61d64ab",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Typical BERT fine-tuning results:\n",
    "- **Accuracy:** ~93‚Äì96% on IMDb reviews  \n",
    "- **Balanced precision/recall**, meaning it handles both positive and negative sentiment equally well.  \n",
    "\n",
    "BERT succeeds because it understands **context and tone** ‚Äî it knows that *‚ÄúThis movie wasn‚Äôt bad at all‚Äù* is positive, not negative.\n",
    "\n",
    "In simple terms:\n",
    "- **Naive Bayes:** counts happy/sad words.  \n",
    "- **Logistic Regression:** weighs those words more intelligently.  \n",
    "- **BERT:** actually understands what the *sentence means.*\n",
    "\n",
    "Next, we‚Äôll compare all three models and summarize what each is best suited for, wrapping up with final conclusions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8920f622",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f052da9c",
   "metadata": {},
   "source": [
    "## Comparison and Final Conclusions\n",
    "\n",
    "Now that we‚Äôve tested all three sentiment analysis models ‚Äî **Naive Bayes**, **Logistic Regression**, and **BERT (PyTorch)** ‚Äî let‚Äôs compare how they performed and what makes each one unique.  \n",
    "Each model represents a milestone in NLP‚Äôs evolution, from simple word counting to deep contextual understanding.\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Model Performance Summary\n",
    "\n",
    "| Model | Accuracy | Precision (avg) | Recall (avg) | F1-Score | Key Strength |\n",
    "|--------|-----------|----------------|---------------|-----------|---------------|\n",
    "| **Naive Bayes** | 0.8655 | 0.87 | 0.87 | 0.87 | Fast, simple, interpretable |\n",
    "| **Logistic Regression** | 0.8958 | 0.90 | 0.90 | 0.90 | Weighted word modeling |\n",
    "| **BERT (PyTorch)** | 0.8540 | 0.86 | 0.85 | 0.85 | Context-aware, deep semantics |\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Observations\n",
    "\n",
    "- **Naive Bayes** performed solidly for its simplicity ‚Äî it‚Äôs still an excellent baseline for text classification.  \n",
    "- **Logistic Regression** delivered the best results here, likely due to the strong TF-IDF features and limited BERT fine-tuning (only two epochs and 10K samples).  \n",
    "- **BERT** underperformed slightly in this small-sample test because fine-tuning large language models requires **more data and training time** to fully adapt to the task.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Interpreting the Results (as if explaining to high school students)\n",
    "\n",
    "- **Naive Bayes** is like counting positive and negative words ‚Äî if a review says ‚Äúamazing‚Äù a lot, it probably likes the movie.  \n",
    "- **Logistic Regression** is smarter ‚Äî it doesn‚Äôt just count; it *learns which words matter more.* It knows that ‚Äúnot bad‚Äù isn‚Äôt the same as ‚Äúbad.‚Äù  \n",
    "- **BERT** reads the review like a person. It doesn‚Äôt rely on counting words ‚Äî it actually *understands context*. But just like a human, it needs more ‚Äúreading practice‚Äù (training) to get really good.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Why the Results Make Sense\n",
    "\n",
    "| Factor | Impact |\n",
    "|---------|--------|\n",
    "| **Limited BERT fine-tuning (10K samples, 2 epochs)** | Not enough time to adapt; underfitting likely. |\n",
    "| **TF-IDF strength for classical models** | Preprocessed word frequencies captured most sentiment clues. |\n",
    "| **Dataset balance** | With 50/50 positive/negative reviews, simpler models already perform well. |\n",
    "| **GPU/epoch constraints** | BERT benefits significantly from longer fine-tuning (3‚Äì5 epochs, full 50K samples). |\n",
    "\n",
    "---\n",
    "\n",
    "### üîß How to Improve BERT for Better Results\n",
    "\n",
    "1. **Fine-tune on full dataset (50K samples)** for at least 3‚Äì4 epochs.  \n",
    "2. **Use DistilBERT or RoBERTa** ‚Äî smaller, faster models often match full BERT accuracy on sentiment tasks.  \n",
    "3. **Add learning rate warmup and gradient clipping** for stability.  \n",
    "4. **Use mixed precision training (AMP)** if running on GPU for faster convergence.  \n",
    "5. **Include data augmentation** (e.g., synonym replacement, random word swap) to expose BERT to more phrasing diversity.\n",
    "\n",
    "---\n",
    "\n",
    "### üìò Portfolio Summary ‚Äî Sentiment Analysis (IMDb Reviews)\n",
    "\n",
    "Built and compared three models for sentiment analysis using IMDb‚Äôs 50K labeled movie reviews:  \n",
    "- **Naive Bayes (86.5%)** ‚Äì simple probability-based baseline using word counts.  \n",
    "- **Logistic Regression (89.6%)** ‚Äì improved accuracy by weighting words intelligently via TF-IDF.  \n",
    "- **BERT (85.4%)** ‚Äì achieved contextual understanding but underperformed due to limited fine-tuning.  \n",
    "\n",
    "This project demonstrates hands-on expertise across **classic NLP pipelines** and **modern transformer-based modeling** using **PyTorch** ‚Äî from preprocessing raw text to evaluating real-world sentiment prediction performance.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like to begin the next portfolio project ‚Äî **Computer Vision (Image Classification)** using CNN, ResNet, and Vision Transformer (ViT)?  \n",
    "We‚Äôll use PyTorch throughout to stay consistent with your framework preference.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
